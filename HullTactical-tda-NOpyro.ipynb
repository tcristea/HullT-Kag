{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === ENV: install libs (run once) ===\n# Use conda if you need GPU wheels for torch; pip below is the simple option.\n! pip install scikit-learn lightgbm xgboost scipy\n! pip install signatory   # preferred signatures backend (requires torch)\n# ! pip install torch       # install matching CUDA build if using GPU (recommended)\n! pip install iisignature # fallback signature lib (C compiled)\n# ! pip install giotto-tda  # optional TDA (persistence + persistence images)\n# Optional (Dionysus sometimes needs conda-forge):\n# conda install -c conda-forge dionysus","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:24:21.501120Z","iopub.execute_input":"2025-11-01T20:24:21.501372Z","iopub.status.idle":"2025-11-01T20:26:20.938838Z","shell.execute_reply.started":"2025-11-01T20:24:21.501349Z","shell.execute_reply":"2025-11-01T20:26:20.938124Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nCollecting signatory\n  Downloading signatory-1.2.6.1.9.0.tar.gz (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: signatory\n  Building wheel for signatory (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for signatory: filename=signatory-1.2.6.1.9.0-cp311-cp311-linux_x86_64.whl size=12019475 sha256=78e67decfe6b1738997cbdca0c8c9093b1b2cfcd020a7be4d2cfbea1050cc3f2\n  Stored in directory: /root/.cache/pip/wheels/6a/79/bb/6012413145dd168da55413ef8bc837f507bf829a08a176c329\nSuccessfully built signatory\nInstalling collected packages: signatory\nSuccessfully installed signatory-1.2.6.1.9.0\nCollecting iisignature\n  Downloading iisignature-0.24.tar.gz (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>1.7 in /usr/local/lib/python3.11/dist-packages (from iisignature) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.7->iisignature) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.7->iisignature) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.7->iisignature) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.7->iisignature) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.7->iisignature) (2024.2.0)\nBuilding wheels for collected packages: iisignature\n  Building wheel for iisignature (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iisignature: filename=iisignature-0.24-cp311-cp311-linux_x86_64.whl size=3246705 sha256=16d328c2bfb73b50d2a5af40d5a26417b2e9161c39da8ad3379983dc4fdf0333\n  Stored in directory: /root/.cache/pip/wheels/1c/f4/57/0b4d3787a07f20a3cd1a91835d6247f55ef899345267bcd6df\nSuccessfully built iisignature\nInstalling collected packages: iisignature\nSuccessfully installed iisignature-0.24\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# === 1) Imports & config ===\nimport os, gc, math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nRANDOM = 42\nN_SPLITS = 5\nSIGNATURE_WINDOW = 30      # days of history for signatures / TDA\nSIGNATURE_DEPTH = 3\nFFT_N = 6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:26:20.940544Z","iopub.execute_input":"2025-11-01T20:26:20.940788Z","iopub.status.idle":"2025-11-01T20:26:25.995208Z","shell.execute_reply.started":"2025-11-01T20:26:20.940765Z","shell.execute_reply":"2025-11-01T20:26:25.994599Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# === 2) Load data & basic cleaning ===\n# Adjust paths to your local files\ntrain = pd.read_csv(\"/kaggle/input/hull-tactical-market-prediction/test.csv\", parse_dates=False)  # date_id is an integer id\ntest  = pd.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\", parse_dates=False)\n\n# mark\ntrain['is_train'] = True\ntest['is_train']  = False\n\n# keep original target if present\nTARGET = 'forward_returns'\n# test may include lagged columns already - keep them as features\n\n# unify columns: ensure both frames have same columns\nmissing_cols = set(train.columns) - set(test.columns)\nfor c in missing_cols:\n    if c not in ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns']: # train-only\n        test[c] = np.nan\n\ndata = pd.concat([train, test], ignore_index=True, sort=False)\n# sort by date_id to guarantee time order\ndata = data.sort_values('date_id').reset_index(drop=True)\n\n# At this stage many early rows may have NaNs. We'll use forward/backfill and median impute later.\nprint(\"Data loaded: rows=\", len(data), \"cols=\", len(data.columns))# === 3) Helper: winsorize by MAD & simple impute ===\nfrom scipy.stats import median_abs_deviation\n\ndef winsorize_mad(series, thresh=4.0):\n    # returns winsorized series\n    med = np.nanmedian(series)\n    mad = median_abs_deviation(series, nan_policy='omit')\n    if mad == 0 or np.isnan(mad):\n        return series.fillna(med)\n    lower = med - thresh * mad\n    upper = med + thresh * mad\n    return series.clip(lower, upper)\n\ndef basic_impute(df, group_col=None, strategy='ffill_mean'):\n    # group_col unused here but left for per-series groups if needed\n    out = df.copy()\n    # forward/backfill then median per-column\n    out = out.fillna(method='ffill').fillna(method='bfill')\n    for c in out.columns:\n        if out[c].isna().any():\n            med = out[c].median(skipna=True)\n            out[c] = out[c].fillna(med)\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:26:25.995892Z","iopub.execute_input":"2025-11-01T20:26:25.996522Z","iopub.status.idle":"2025-11-01T20:26:26.330458Z","shell.execute_reply.started":"2025-11-01T20:26:25.996500Z","shell.execute_reply":"2025-11-01T20:26:26.329715Z"}},"outputs":[{"name":"stdout","text":"Data loaded: rows= 9000 cols= 103\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === 4) Conventional features: lags, rolling and FFT ===\ndef add_basic_ts_features(df, value_prefixes=('M','E','I','P','V','S','MOM','D'), \n                          windows=(5,20,60), fft_n=FFT_N):\n    \"\"\"\n    For each numeric column whose name starts with prefix, compute lags and rolling stats.\n    We compute FFT on the multivariate vector for each date using a trailing window across all features.\n    \"\"\"\n    df = df.copy()\n    # pick feature columns by prefix (preserve provided lagged_* too)\n    candidate_cols = [c for c in df.columns if any(c.startswith(pref) for pref in value_prefixes)]\n    # include lagged_* if present\n    candidate_cols += [c for c in df.columns if c.startswith('lagged_')]\n    candidate_cols = sorted(set(candidate_cols) & set(df.columns))\n\n    # create shifted lags and rolling per column (trailing windows)\n    for w in windows:\n        for c in candidate_cols:\n            df[f'{c}_lag_{w}'] = df[c].shift(w)\n            df[f'{c}_rollmean_{w}'] = df[c].shift(1).rolling(window=w).mean().reset_index(drop=True)\n            df[f'{c}_rollstd_{w}'] = df[c].shift(1).rolling(window=w).std().reset_index(drop=True)\n    # FFT features: for each date, take the trailing window of length max(windows) across candidate_cols\n    max_w = max(windows)\n    fft_cols = [f'fft_{i}' for i in range(1, fft_n+1)]\n    # create empty columns\n    for fc in fft_cols:\n        df[fc] = np.nan\n    # compute FFT magnitudes of flattened multivariate trailing vector (may be heavy)\n    # optimized: compute per-row using rolling index\n    values = df[candidate_cols].values\n    for i in range(len(df)):\n        s = max(0, i - max_w + 1)\n        window_block = values[s:i+1]  # shape (L, n_features)\n        if window_block.shape[0] < 3:\n            continue\n        # flatten along time axis to 1D signal\n        flat = window_block.ravel()\n        arr = np.fft.rfft(np.nan_to_num(flat))\n        mags = np.abs(arr)\n        # assign first FFT_N non-dc mags\n        for k in range(1, min(len(mags), fft_n+1)):\n            df.at[i, f'fft_{k}'] = mags[k]\n    # winsorize numeric columns to reduce extreme outliers\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    for c in numeric_cols:\n        df[c] = winsorize_mad(df[c], thresh=4.0)\n    # basic final impute\n    df = basic_impute(df)\n    return df\n\n# usage:\ndata = add_basic_ts_features(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:26:41.138888Z","iopub.execute_input":"2025-11-01T20:26:41.139500Z","iopub.status.idle":"2025-11-01T20:26:47.229744Z","shell.execute_reply.started":"2025-11-01T20:26:41.139476Z","shell.execute_reply":"2025-11-01T20:26:47.229162Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# === 5) Signatures: sliding-window per date_id using signatory or iisignature ===\n# Signatures will produce one feature vector per date_id (signature of preceding SIGNATURE_WINDOW rows of selected features)\n\n# preferred: signatory (GPU+torch); fallback: iisignature (C)\ntry:\n    import torch, signatory\n    SIGNATORY_OK = True\nexcept Exception:\n    SIGNATORY_OK = False\n\ntry:\n    import iisignature\n    IISIGNATURE_OK = True\nexcept Exception:\n    IISIGNATURE_OK = False\n\ndef compute_signatures_by_date(df, feature_cols, window=SIGNATURE_WINDOW, depth=SIGNATURE_DEPTH, id_col='date_id'):\n    \"\"\"\n    Returns DataFrame with columns [date_id, sig_0, sig_1, ...] where each row is the signature\n    of the trailing window of 'feature_cols' ending at that date_id.\n    If insufficient history, signature is NaN and will be dropped later.\n    \"\"\"\n    sig_rows = []\n    # ensure sorted\n    df = df.sort_values(id_col).reset_index(drop=True)\n    feats = df[feature_cols].values.astype(np.float32)\n    n, d = feats.shape\n    if SIGNATORY_OK:\n        # compute per-date sliding window in a loop (vectorization possible but more complex)\n        for i in range(n):\n            start = i - window + 1\n            if start < 0:\n                sig_rows.append((df.at[i, id_col], None))\n                continue\n            seg = torch.tensor(feats[start:i+1][None], device='cpu')  # shape (1, L, d)\n            try:\n                sig = signatory.signature(seg, depth=depth)[0].cpu().numpy()\n            except Exception:\n                sig = None\n            sig_rows.append((df.at[i, id_col], sig))\n    elif IISIGNATURE_OK:\n        for i in range(n):\n            start = i - window + 1\n            if start < 0:\n                sig_rows.append((df.at[i, id_col], None))\n                continue\n            seg = feats[start:i+1]\n            try:\n                sig = iisignature.sig(seg, depth)\n            except Exception:\n                sig = None\n            sig_rows.append((df.at[i, id_col], sig))\n    else:\n        raise RuntimeError(\"No signature backend available (install signatory or iisignature).\")\n\n    # build DataFrame, drop None rows\n    valid = [(idx, s) for idx, s in sig_rows if s is not None]\n    if len(valid) == 0:\n        raise RuntimeError(\"No signatures computed (insufficient history). Consider reducing window.\")\n    feat_dim = valid[0][1].shape[0]\n    cols = ['date_id'] + [f'sig_{i}' for i in range(feat_dim)]\n    out = pd.DataFrame([ [idx] + list(s) for idx, s in valid ], columns=cols)\n    return out\n\n# Usage:\nselected_cols = [c for c in data.columns if c.startswith(('M','E','I','P','V','S','MOM'))][:10]  # choose top K to limit dimension\nsig_df = compute_signatures_by_date(data, selected_cols, window=30, depth=3)\ndata = data.merge(sig_df, on='date_id', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:26:54.206502Z","iopub.execute_input":"2025-11-01T20:26:54.206786Z","iopub.status.idle":"2025-11-01T20:27:02.727189Z","shell.execute_reply.started":"2025-11-01T20:26:54.206765Z","shell.execute_reply":"2025-11-01T20:27:02.726523Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# === 6) Optional TDA (giotto-tda): persistence image per trailing window ===\n# This block is optional and slower. It computes a persistence image for each trailing window of multivariate features.\n\ntry:\n    from gtda.homology import VietorisRipsPersistence\n    from gtda.diagrams import PersistenceImage\n    GIOTTO_OK = True\nexcept Exception:\n    GIOTTO_OK = False\n\ndef compute_persistence_images_by_date(df, feature_cols, window=SIGNATURE_WINDOW, id_col='date_id'):\n    if not GIOTTO_OK:\n        raise RuntimeError(\"giotto-tda not installed. Install or skip TDA.\")\n    pr = VietorisRipsPersistence(metric='euclidean', homology_dimensions=[0,1])\n    pimg = PersistenceImage()\n    rows = []\n    df = df.sort_values(id_col).reset_index(drop=True)\n    feats = df[feature_cols].values\n    n = len(df)\n    for i in range(n):\n        start = i - window + 1\n        if start < 0:\n            rows.append((df.at[i, id_col], None))\n            continue\n        block = feats[start:i+1]\n        if block.shape[0] < 3:\n            rows.append((df.at[i, id_col], None))\n            continue\n        diagrams = pr.fit_transform(block[None])    # returns array shape (1, ... )\n        image = pimg.fit_transform(diagrams)[0].ravel()\n        rows.append((df.at[i, id_col], image))\n    valid = [(idx,img) for idx,img in rows if img is not None]\n    if len(valid) == 0:\n        raise RuntimeError(\"No persistence images computed.\")\n    dim = valid[0][1].shape[0]\n    cols = ['date_id'] + [f'pi_{i}' for i in range(dim)]\n    out = pd.DataFrame([ [idx] + list(img) for idx,img in valid ], columns=cols)\n    return out\n\n# Usage:\n# pi_df = compute_persistence_images_by_date(data, selected_cols, window=30)\n# data = data.merge(pi_df, on='date_id', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:27:07.068463Z","iopub.execute_input":"2025-11-01T20:27:07.069301Z","iopub.status.idle":"2025-11-01T20:27:07.077756Z","shell.execute_reply.started":"2025-11-01T20:27:07.069272Z","shell.execute_reply":"2025-11-01T20:27:07.076981Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# === 7) Assemble final training set and features list ===\ndef prepare_features(data, signature_cols_keep_k=200):\n    df = data.copy()\n    # Basic features: all numeric except target and identifiers\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    ignore = ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns', 'is_train']\n    numeric_cols = [c for c in numeric_cols if c not in ignore]\n\n    # Optionally pick top-K signature columns (sig_*)\n    sig_cols = [c for c in numeric_cols if c.startswith('sig_')]\n    if len(sig_cols) > signature_cols_keep_k:\n        # simple heuristic: keep first K\n        sig_cols = sig_cols[:signature_cols_keep_k]\n\n    pi_cols = [c for c in numeric_cols if c.startswith('pi_')]\n    # final features: numeric base + signatures + persistence images\n    base = [c for c in numeric_cols if not (c.startswith('sig_') or c.startswith('pi_'))]\n    features = base + sig_cols + pi_cols\n    # remove date_id if present\n    if 'date_id' in features:\n        features.remove('date_id')\n    return df, features\n\n# usage:\n# after computing signatures/tda and merging into data:\ndata, features = prepare_features(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:27:11.824400Z","iopub.execute_input":"2025-11-01T20:27:11.825132Z","iopub.status.idle":"2025-11-01T20:27:11.914247Z","shell.execute_reply.started":"2025-11-01T20:27:11.825081Z","shell.execute_reply":"2025-11-01T20:27:11.913464Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# === Neural Network Model Definitions ===\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nimport tensorflow as tf\n\ndef build_deep_residual_model(input_dim):\n    \"\"\"Deep Residual Network with Dropout\"\"\"\n    inputs = keras.Input(shape=(input_dim,))\n    \n    # Initial dense layer\n    x = layers.Dense(256, activation='relu')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Residual blocks\n    for units in [128, 128, 64]:\n        residual = x\n        x = layers.Dense(units, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.2)(x)\n        x = layers.Dense(units, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        if residual.shape[-1] == units:\n            x = layers.Add()([x, residual])\n        x = layers.Dropout(0.2)(x)\n    \n    # Output\n    outputs = layers.Dense(1)(x)\n    \n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse', metrics=['mae'])\n    return model\n\ndef build_wide_deep_model(input_dim):\n    \"\"\"Wide & Deep Architecture\"\"\"\n    inputs = keras.Input(shape=(input_dim,))\n    \n    # Wide component (linear)\n    wide = layers.Dense(32, activation='relu')(inputs)\n    \n    # Deep component\n    deep = layers.Dense(256, activation='relu')(inputs)\n    deep = layers.BatchNormalization()(deep)\n    deep = layers.Dropout(0.3)(deep)\n    deep = layers.Dense(128, activation='relu')(deep)\n    deep = layers.BatchNormalization()(deep)\n    deep = layers.Dropout(0.2)(deep)\n    deep = layers.Dense(64, activation='relu')(deep)\n    deep = layers.Dropout(0.2)(deep)\n    \n    # Combine wide and deep\n    combined = layers.Concatenate()([wide, deep])\n    combined = layers.Dense(32, activation='relu')(combined)\n    outputs = layers.Dense(1)(combined)\n    \n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse', metrics=['mae'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:27:22.805334Z","iopub.execute_input":"2025-11-01T20:27:22.805849Z","iopub.status.idle":"2025-11-01T20:27:36.188375Z","shell.execute_reply.started":"2025-11-01T20:27:22.805822Z","shell.execute_reply":"2025-11-01T20:27:36.187592Z"}},"outputs":[{"name":"stderr","text":"2025-11-01 20:27:24.474286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762028844.712021      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762028844.782515      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# === 8) Model training: TimeSeries CV + DNN + XGB OOF ===\ndef train_oof_dnn_xgb(df, features, target='forward_returns', date_col='date_id', n_splits=N_SPLITS, gpu=True):\n    \"\"\"Train DNN and XGBoost with out-of-fold predictions\"\"\"\n    df = df.sort_values(date_col).reset_index(drop=True)\n    idx = df.index.values\n    n = len(df)\n    tss = TimeSeriesSplit(n_splits=n_splits)\n    \n    oof_dnn = np.zeros(n)\n    oof_xgb = np.zeros(n)\n    \n    # XGBoost params\n    xgb_params = dict(\n        objective='reg:squarederror', \n        eval_metric='rmse', \n        learning_rate=0.02,\n        max_depth=8, \n        subsample=0.8, \n        colsample_bytree=0.8, \n        tree_method='gpu_hist' if gpu else 'hist',\n        verbose=5\n    )\n    \n    # Only train on rows that have target (train portion)\n    train_mask = ~df[target].isna()\n    train_idx = np.where(train_mask)[0]\n    splits = list(tss.split(train_idx))\n    print('show splits#############', len(splits))\n    \n    for fold, (t_idx_local, v_idx_local) in enumerate(splits):\n        print(f\"\\n=== Fold {fold+1}/{len(splits)} ===\")\n        t_idx = train_idx[t_idx_local]\n        v_idx = train_idx[v_idx_local]\n        \n        X_tr, X_val = df.iloc[t_idx][features], df.iloc[v_idx][features]\n        y_tr, y_val = df.iloc[t_idx][target].values, df.iloc[v_idx][target].values\n        \n        # === Deep Neural Network ===\n        print(\"Training DNN...\")\n        model_dnn = build_deep_residual_model(len(features))\n        \n        early_stop = callbacks.EarlyStopping(\n            monitor='val_loss', \n            patience=30, \n            restore_best_weights=True,\n            verbose=1\n        )\n        reduce_lr = callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.5, \n            patience=15, \n            min_lr=1e-6,\n            verbose=1\n        )\n        \n        history = model_dnn.fit(\n            X_tr.values, y_tr,\n            validation_data=(X_val.values, y_val),\n            epochs=500,\n            batch_size=256,\n            callbacks=[early_stop, reduce_lr],\n            verbose=0\n        )\n        \n        oof_dnn[v_idx] = model_dnn.predict(X_val.values, verbose=0).flatten()\n        print(f\"DNN Fold {fold+1} val loss: {min(history.history['val_loss']):.6f}\")\n        \n        # === XGBoost ===\n        print(\"Training XGBoost...\")\n        dtrain_x = xgb.DMatrix(X_tr, label=y_tr)\n        dvalid_x = xgb.DMatrix(X_val, label=y_val)\n        \n        model_xgb = xgb.train(\n            xgb_params, \n            dtrain_x, \n            num_boost_round=10000, \n            evals=[(dvalid_x, 'valid')],\n            early_stopping_rounds=200,\n            verbose_eval=500\n        )\n        \n        oof_xgb[v_idx] = model_xgb.predict(dvalid_x, iteration_range=(0, model_xgb.best_iteration))\n        \n        # Cleanup\n        del model_dnn, model_xgb, dtrain_x, dvalid_x\n        keras.backend.clear_session()\n        gc.collect()\n    \n    # Blend simple average for OOF evaluation\n    oof_blend = 0.5 * oof_dnn + 0.5 * oof_xgb\n    \n    # Compute OOF on training rows\n    oof_rmse = mean_squared_error(df.loc[train_idx, target], oof_blend[train_idx], squared=False)\n    print(\"\\n\" + \"=\"*50)\n    print(f\"OOF RMSE (DNN): {mean_squared_error(df.loc[train_idx, target], oof_dnn[train_idx], squared=False):.6f}\")\n    print(f\"OOF RMSE (XGB): {mean_squared_error(df.loc[train_idx, target], oof_xgb[train_idx], squared=False):.6f}\")\n    print(f\"OOF RMSE (blend): {oof_rmse:.6f}\")\n    print(\"=\"*50)\n    \n    return oof_dnn, oof_xgb, oof_blend, oof_rmse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:27:40.435189Z","iopub.execute_input":"2025-11-01T20:27:40.436509Z","iopub.status.idle":"2025-11-01T20:27:40.447177Z","shell.execute_reply.started":"2025-11-01T20:27:40.436480Z","shell.execute_reply":"2025-11-01T20:27:40.446394Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# === 9) Meta-stacker (Ridge) and full-train/predict helpers ===\ndef train_meta_and_full_predict(df, features, oof_preds, target='forward_returns', date_col='date_id', gpu=True):\n    \"\"\"Train meta-model (Ridge) and full models for final predictions\"\"\"\n    \n    df = df.sort_values(date_col).reset_index(drop=True)\n    train_mask = ~df[target].isna()\n    train_idx = np.where(train_mask)[0]\n    \n    # Prepare meta features\n    if isinstance(oof_preds, dict):\n        X_meta = np.vstack([oof_preds[k] for k in sorted(oof_preds.keys())]).T\n    else:\n        X_meta = oof_preds\n    \n    # Train meta-model (Ridge)\n    print(\"\\n=== Training Meta-Model (Ridge) ===\")\n    meta = Ridge(alpha=1.0)\n    meta.fit(X_meta[train_idx], df.loc[train_idx, target].values)\n    oof_meta = meta.predict(X_meta)\n    meta_rmse = mean_squared_error(df.loc[train_idx, target].values, oof_meta[train_idx], squared=False)\n    print(f\"Meta OOF RMSE: {meta_rmse:.6f}\")\n    \n    # Full-train on all training rows\n    X_full = df.loc[train_idx, features]\n    y_full = df.loc[train_idx, target].values\n    \n    # === Wide & Deep Neural Network (full train) ===\n    print(\"\\n=== Training Full Wide-Deep DNN ===\")\n    model_dnn = build_wide_deep_model(len(features))\n    \n    reduce_lr = callbacks.ReduceLROnPlateau(\n        monitor='loss', \n        factor=0.5, \n        patience=20, \n        min_lr=1e-6,\n        verbose=1\n    )\n    \n    history = model_dnn.fit(\n        X_full.values, y_full,\n        epochs=300,\n        batch_size=256,\n        callbacks=[reduce_lr],\n        verbose=1\n    )\n    \n    # === XGBoost (full train) ===\n    print(\"\\n=== Training Full XGBoost ===\")\n    xgb_params = dict(\n        objective='reg:squarederror', \n        eval_metric='rmse', \n        learning_rate=0.02,\n        max_depth=8, \n        subsample=0.8, \n        colsample_bytree=0.8, \n        tree_method='gpu_hist' if gpu else 'hist'\n    )\n    dtrain = xgb.DMatrix(X_full, label=y_full)\n    model_xgb = xgb.train(xgb_params, dtrain, num_boost_round=2000, verbose_eval=500)\n    \n    # Predict for all rows (train + test)\n    print(\"\\n=== Generating Predictions ===\")\n    all_X = df[features]\n    \n    pred_dnn_all = model_dnn.predict(all_X.values, verbose=0).flatten()\n    pred_xgb_all = model_xgb.predict(xgb.DMatrix(all_X))\n    \n    # Stack and meta-predict\n    stack_input = np.vstack([pred_dnn_all, pred_xgb_all]).T\n    pred_meta_all = meta.predict(stack_input)\n    \n    # Add predictions to dataframe\n    df['pred_dnn'] = pred_dnn_all\n    df['pred_xgb'] = pred_xgb_all\n    df['pred_meta'] = pred_meta_all\n    \n    print(\"\\n=== Training Complete ===\")\n    return df, model_dnn, model_xgb, meta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:27:46.764517Z","iopub.execute_input":"2025-11-01T20:27:46.764771Z","iopub.status.idle":"2025-11-01T20:27:46.773813Z","shell.execute_reply.started":"2025-11-01T20:27:46.764752Z","shell.execute_reply":"2025-11-01T20:27:46.773142Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# === TRAINING CELL - runs once when notebook starts ===\nimport joblib\nimport gc\n\ndef train_and_save_ensemble():\n    \"\"\"Complete training pipeline with DNN + XGBoost + Ridge meta-model\"\"\"\n    \n    print(\"=\"*60)\n    print(\"Starting Ensemble Training Pipeline\")\n    print(\"=\"*60)\n    \n    # 1. Feature Engineering\n    print(\"\\n=== Step 1: Feature Engineering ===\")\n    data_fe = add_basic_ts_features(data, windows=(5,20,60), fft_n=FFT_N)\n    \n    # Feature candidates\n    feat_candidates = [c for c in data_fe.columns if c.startswith(('M','E','I','P','V','S','MOM'))]\n    feat_candidates = sorted(feat_candidates)[:12]\n    \n    # 2. Signatures (optional)\n    print(\"\\n=== Step 2: Computing Signatures ===\")\n    try:\n        sig_df = compute_signatures_by_date(\n            data_fe, \n            feat_candidates, \n            window=SIGNATURE_WINDOW, \n            depth=SIGNATURE_DEPTH\n        )\n        data_fe = data_fe.merge(sig_df, on='date_id', how='left')\n        print(\"Signatures computed successfully\")\n    except Exception as e:\n        print(f\"Signature step skipped: {e}\")\n    \n    # 3. Final feature preparation\n    print(\"\\n=== Step 3: Preparing Features ===\")\n    data_final, features = prepare_features(data_fe, signature_cols_keep_k=200)\n    print(f\"Total features: {len(features)}\")\n    \n    # 4. Train OOF models (DNN + XGBoost)\n    print(\"\\n=== Step 4: Training OOF Models ===\")\n    oof_dnn, oof_xgb, oof_blend, oof_rmse = train_oof_dnn_xgb(\n        data_final, \n        features, \n        target=TARGET, \n        n_splits=N_SPLITS, \n        gpu=True\n    )\n    \n    # 5. Meta-stacking with Ridge\n    print(\"\\n=== Step 5: Meta-Stacking ===\")\n    oof_preds = {'dnn': oof_dnn, 'xgb': oof_xgb}\n    df_preds, model_dnn, model_xgb, meta = train_meta_and_full_predict(\n        data_final, \n        features, \n        oof_preds, \n        target=TARGET, \n        gpu=True\n    )\n    \n    # 6. Save entire ensemble\n    print(\"\\n=== Step 6: Saving Ensemble ===\")\n    ensemble = {\n        'model_dnn': model_dnn,\n        'model_xgb': model_xgb, \n        'meta_model': meta,\n        'features': features,\n        'feature_processor': None,  # Add if you have feature preprocessing\n        'data_final': data_final,   # For reference\n        'oof_rmse': oof_rmse\n    }\n    \n    joblib.dump(ensemble, 'ensemble_model.pkl')\n    print(\"\\n\" + \"=\"*60)\n    print(\"Ensemble trained and saved successfully!\")\n    print(f\"Final OOF RMSE: {oof_rmse:.6f}\")\n    print(\"=\"*60)\n    \n    return ensemble\n\n# Train immediately\ntrained_ensemble = train_and_save_ensemble()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T20:27:54.575417Z","iopub.execute_input":"2025-11-01T20:27:54.575691Z","iopub.status.idle":"2025-11-01T20:37:24.732823Z","shell.execute_reply.started":"2025-11-01T20:27:54.575670Z","shell.execute_reply":"2025-11-01T20:37:24.731740Z"}},"outputs":[{"name":"stdout","text":"============================================================\nStarting Ensemble Training Pipeline\n============================================================\n\n=== Step 1: Feature Engineering ===\n\n=== Step 2: Computing Signatures ===\nSignatures computed successfully\n\n=== Step 3: Preparing Features ===\nTotal features: 9034\n\n=== Step 4: Training OOF Models ===\nshow splits############# 5\n\n=== Fold 1/5 ===\nTraining DNN...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1762028946.235249      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1762028946.235998      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1762028955.292653     169 service.cc:148] XLA service 0x7e036001ec60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1762028955.293465     169 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1762028955.293496     169 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1762028956.008427     169 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1762028960.706212     169 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 31: early stopping\nRestoring model weights from the end of the best epoch: 1.\nDNN Fold 1 val loss: nan\nTraining XGBoost...\n[0]\tvalid-rmse:0.01073\n[199]\tvalid-rmse:0.01085\n\n=== Fold 2/5 ===\nTraining DNN...\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 31: early stopping\nRestoring model weights from the end of the best epoch: 1.\nDNN Fold 2 val loss: nan\nTraining XGBoost...\n[0]\tvalid-rmse:0.00897\n[204]\tvalid-rmse:0.00907\n\n=== Fold 3/5 ===\nTraining DNN...\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 31: early stopping\nRestoring model weights from the end of the best epoch: 1.\nDNN Fold 3 val loss: nan\nTraining XGBoost...\n[0]\tvalid-rmse:0.01034\n[228]\tvalid-rmse:0.01048\n\n=== Fold 4/5 ===\nTraining DNN...\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 31: early stopping\nRestoring model weights from the end of the best epoch: 1.\nDNN Fold 4 val loss: nan\nTraining XGBoost...\n[0]\tvalid-rmse:0.00784\n[204]\tvalid-rmse:0.00818\n\n=== Fold 5/5 ===\nTraining DNN...\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 31: early stopping\nRestoring model weights from the end of the best epoch: 1.\nDNN Fold 5 val loss: nan\nTraining XGBoost...\n[0]\tvalid-rmse:0.00930\n[200]\tvalid-rmse:0.00943\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3465845309.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Train immediately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mtrained_ensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_save_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/3465845309.py\u001b[0m in \u001b[0;36mtrain_and_save_ensemble\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# 4. Train OOF models (DNN + XGBoost)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Step 4: Training OOF Models ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     oof_dnn, oof_xgb, oof_blend, oof_rmse = train_oof_dnn_xgb(\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mdata_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2734466587.py\u001b[0m in \u001b[0;36mtrain_oof_dnn_xgb\u001b[0;34m(df, features, target, date_col, n_splits, gpu)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Compute OOF on training rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0moof_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_blend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OOF RMSE (DNN): {mean_squared_error(df.loc[train_idx, target], oof_dnn[train_idx], squared=False):.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;36m0.825\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input contains NaN."],"ename":"ValueError","evalue":"Input contains NaN.","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# === INFERENCE SETUP ===\nimport os\nimport pandas as pd\nimport polars as pl\nimport kaggle_evaluation.default_inference_server\nimport numpy as np\nimport joblib\n\n# Global ensemble cache\ncached_ensemble = None\n\ndef load_ensemble():\n    \"\"\"Load pre-trained ensemble - called once on first predict\"\"\"\n    global cached_ensemble\n    if cached_ensemble is None:\n        try:\n            cached_ensemble = joblib.load('ensemble_model.pkl')\n            print(\"Ensemble loaded successfully!\")\n        except Exception as e:\n            print(f\"Failed to load from file: {e}\")\n            try:\n                cached_ensemble = trained_ensemble\n                print(\"Using in-memory trained ensemble\")\n            except:\n                print(\"ERROR: No ensemble available!\")\n                raise\n    return cached_ensemble\n\ndef predict(test: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Competition inference function - predicts positions [0, 2]\"\"\"\n    \n    ensemble = load_ensemble()\n    \n    # Convert to pandas\n    test_pd = test.to_pandas()\n    \n    try:\n        # Ensure we have the same features (in same order)\n        features = ensemble['features']\n        \n        # Get feature matrix (handle missing features gracefully)\n        if set(features).issubset(test_pd.columns):\n            X_test = test_pd[features].copy()\n        else:\n            # Create DataFrame with available features\n            X_test = test_pd[[f for f in features if f in test_pd.columns]].copy()\n            \n            # Add missing features with zeros\n            missing_features = set(features) - set(X_test.columns)\n            for f in missing_features:\n                X_test[f] = 0.0\n        \n        # Ensure correct feature order\n        X_test = X_test[features]\n        \n        # Get base model predictions\n        pred_dnn = ensemble['model_dnn'].predict(X_test.values, verbose=0).flatten()\n        pred_xgb = ensemble['model_xgb'].predict(xgb.DMatrix(X_test))\n        \n        # Meta-stack with Ridge\n        stack_input = np.column_stack([pred_dnn, pred_xgb])\n        final_predictions = ensemble['meta_model'].predict(stack_input)\n        \n        # Convert returns to positions [0, 2]\n        # Simple scaling: shift and clip\n        # Adjust this based on your expected return distribution\n        positions = np.clip(final_predictions * 10 + 1, 0, 2)\n        \n    except Exception as e:\n        print(f\"Inference error: {e}, using fallback\")\n        # Fallback: neutral position\n        positions = np.ones(len(test_pd)) * 1.0\n    \n    return pl.DataFrame({'prediction': positions})\n\n# Test inference function (optional)\ndef test_inference():\n    \"\"\"Quick test of inference pipeline\"\"\"\n    print(\"\\n=== Testing Inference ===\")\n    ensemble = load_ensemble()\n    \n    # Create dummy test data\n    n_test = 100\n    test_data = pd.DataFrame(\n        np.random.randn(n_test, len(ensemble['features'])),\n        columns=ensemble['features']\n    )\n    test_pl = pl.from_pandas(test_data)\n    \n    # Test prediction\n    result = predict(test_pl)\n    print(f\"Generated {len(result)} predictions\")\n    print(f\"Prediction range: [{result['prediction'].min():.4f}, {result['prediction'].max():.4f}]\")\n    print(f\"Mean prediction: {result['prediction'].mean():.4f}\")\n    print(\"Inference test completed successfully!\")\n\n# Uncomment to test inference\n# test_inference()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T12:25:41.982177Z","iopub.execute_input":"2025-10-09T12:25:41.982478Z","iopub.status.idle":"2025-10-09T12:25:42.773984Z","shell.execute_reply.started":"2025-10-09T12:25:41.982459Z","shell.execute_reply":"2025-10-09T12:25:42.773263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SERVER STARTUP (same as before)\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T12:25:48.185379Z","iopub.execute_input":"2025-10-09T12:25:48.185974Z","iopub.status.idle":"2025-10-09T12:27:40.491176Z","shell.execute_reply.started":"2025-10-09T12:25:48.185948Z","shell.execute_reply":"2025-10-09T12:27:40.490445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pl.read_parquet(\"/kaggle/working/submission.parquet\").head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T12:29:15.772271Z","iopub.execute_input":"2025-10-09T12:29:15.772549Z","iopub.status.idle":"2025-10-09T12:29:15.779812Z","shell.execute_reply.started":"2025-10-09T12:29:15.772528Z","shell.execute_reply":"2025-10-09T12:29:15.778983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}