{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === ENV: install libs (run once) ===\n# Use conda if you need GPU wheels for torch; pip below is the simple option.\n! pip install scikit-learn lightgbm xgboost scipy\n! pip install signatory   # preferred signatures backend (requires torch)\n# ! pip install torch       # install matching CUDA build if using GPU (recommended)\n! pip install iisignature # fallback signature lib (C compiled)\n# ! pip install giotto-tda  # optional TDA (persistence + persistence images)\n# Optional (Dionysus sometimes needs conda-forge):\n# conda install -c conda-forge dionysus","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T17:27:45.854090Z","iopub.execute_input":"2025-11-06T17:27:45.854936Z","iopub.status.idle":"2025-11-06T17:30:18.978596Z","shell.execute_reply.started":"2025-11-06T17:27:45.854911Z","shell.execute_reply":"2025-11-06T17:30:18.977468Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nCollecting signatory\n  Downloading signatory-1.2.6.1.9.0.tar.gz (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: signatory\n  Building wheel for signatory (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for signatory: filename=signatory-1.2.6.1.9.0-cp311-cp311-linux_x86_64.whl size=12019475 sha256=501aa517cfdd37ad85e69a45112a8b32ea64e16c9e722b6a93055b82e2e7e452\n  Stored in directory: /root/.cache/pip/wheels/6a/79/bb/6012413145dd168da55413ef8bc837f507bf829a08a176c329\nSuccessfully built signatory\nInstalling collected packages: signatory\nSuccessfully installed signatory-1.2.6.1.9.0\nCollecting iisignature\n  Downloading iisignature-0.24.tar.gz (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>1.7 in /usr/local/lib/python3.11/dist-packages (from iisignature) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.7->iisignature) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.7->iisignature) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.7->iisignature) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.7->iisignature) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.7->iisignature) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.7->iisignature) (2024.2.0)\nBuilding wheels for collected packages: iisignature\n  Building wheel for iisignature (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iisignature: filename=iisignature-0.24-cp311-cp311-linux_x86_64.whl size=3246704 sha256=23c6fc114c934fcda8447b28a50d62d161cd92eac0f3f483d0ae8ab716116cde\n  Stored in directory: /root/.cache/pip/wheels/1c/f4/57/0b4d3787a07f20a3cd1a91835d6247f55ef899345267bcd6df\nSuccessfully built iisignature\nInstalling collected packages: iisignature\nSuccessfully installed iisignature-0.24\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# === 1) Imports & config ===\nimport os, gc, math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nRANDOM = 42\nN_SPLITS = 5\nSIGNATURE_WINDOW = 30      # days of history for signatures / TDA\nSIGNATURE_DEPTH = 3\nFFT_N = 6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T17:30:18.982796Z","iopub.execute_input":"2025-11-06T17:30:18.983096Z","iopub.status.idle":"2025-11-06T17:30:25.163776Z","shell.execute_reply.started":"2025-11-06T17:30:18.983059Z","shell.execute_reply":"2025-11-06T17:30:25.163065Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# === 2) Load data & basic cleaning ===\n# Adjust paths to your local files\ntrain = pd.read_csv(\"/kaggle/input/hull-tactical-market-prediction/test.csv\", parse_dates=False)  # date_id is an integer id\ntest  = pd.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\", parse_dates=False)\n\n# mark\ntrain['is_train'] = True\ntest['is_train']  = False\n\n# keep original target if present\nTARGET = 'forward_returns'\n# test may include lagged columns already - keep them as features\n\n# unify columns: ensure both frames have same columns\nmissing_cols = set(train.columns) - set(test.columns)\nfor c in missing_cols:\n    if c not in ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns']: # train-only\n        test[c] = np.nan\n\ndata = pd.concat([train, test], ignore_index=True, sort=False)\n# sort by date_id to guarantee time order\ndata = data.sort_values('date_id').reset_index(drop=True)\n\n# At this stage many early rows may have NaNs. We'll use forward/backfill and median impute later.\nprint(\"Data loaded: rows=\", len(data), \"cols=\", len(data.columns))# === 3) Helper: winsorize by MAD & simple impute ===\nfrom scipy.stats import median_abs_deviation\n\ndef winsorize_mad(series, thresh=4.0):\n    # returns winsorized series\n    med = np.nanmedian(series)\n    mad = median_abs_deviation(series, nan_policy='omit')\n    if mad == 0 or np.isnan(mad):\n        return series.fillna(med)\n    lower = med - thresh * mad\n    upper = med + thresh * mad\n    return series.clip(lower, upper)\n\ndef basic_impute(df, group_col=None, strategy='ffill_mean'):\n    out = df.copy()\n    out = out.fillna(method='ffill').fillna(method='bfill')\n    \n    for c in out.columns:\n        if out[c].isna().any():\n            # Check if ALL values are NaN after ffill/bfill\n            if out[c].isna().all():\n                out[c] = 0  # or some other default value\n            else:\n                med = out[c].median(skipna=True)\n                out[c] = out[c].fillna(med)\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T17:30:39.476840Z","iopub.execute_input":"2025-11-06T17:30:39.477466Z","iopub.status.idle":"2025-11-06T17:30:39.828479Z","shell.execute_reply.started":"2025-11-06T17:30:39.477440Z","shell.execute_reply":"2025-11-06T17:30:39.827715Z"}},"outputs":[{"name":"stdout","text":"Data loaded: rows= 9031 cols= 103\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"data.isna().sum().sum(), train.isna().sum().sum(), test.isna().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T17:31:14.527739Z","iopub.execute_input":"2025-11-06T17:31:14.528018Z","iopub.status.idle":"2025-11-06T17:31:14.545125Z","shell.execute_reply.started":"2025-11-06T17:31:14.528000Z","shell.execute_reply":"2025-11-06T17:31:14.544242Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(173789, 0, 173759)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# === 4) Conventional features: lags, rolling and FFT ===\ndef add_basic_ts_features(df, value_prefixes=('M','E','I','P','V','S','MOM','D'), \n                          windows=(5,20,60), fft_n=FFT_N):\n    \"\"\"\n    For each numeric column whose name starts with prefix, compute lags and rolling stats.\n    We compute FFT on the multivariate vector for each date using a trailing window across all features.\n    \"\"\"\n    df = df.copy()\n    # pick feature columns by prefix (preserve provided lagged_* too)\n    candidate_cols = [c for c in df.columns if any(c.startswith(pref) for pref in value_prefixes)]\n    # include lagged_* if present\n    candidate_cols += [c for c in df.columns if c.startswith('lagged_')]\n    candidate_cols = sorted(set(candidate_cols) & set(df.columns))\n\n    # create shifted lags and rolling per column (trailing windows)\n    for w in windows:\n        for c in candidate_cols:\n            df[f'{c}_lag_{w}'] = df[c].shift(w)\n            df[f'{c}_rollmean_{w}'] = df[c].shift(1).rolling(window=w).mean().reset_index(drop=True)\n            df[f'{c}_rollstd_{w}'] = df[c].shift(1).rolling(window=w).std().reset_index(drop=True)\n    # FFT features: for each date, take the trailing window of length max(windows) across candidate_cols\n    max_w = max(windows)\n    fft_cols = [f'fft_{i}' for i in range(1, fft_n+1)]\n    # create empty columns\n    for fc in fft_cols:\n        df[fc] = np.nan\n    # compute FFT magnitudes of flattened multivariate trailing vector (may be heavy)\n    # optimized: compute per-row using rolling index\n    values = df[candidate_cols].values\n    for i in range(len(df)):\n        s = max(0, i - max_w + 1)\n        window_block = values[s:i+1]  # shape (L, n_features)\n        if window_block.shape[0] < 3:\n            continue\n        # flatten along time axis to 1D signal\n        flat = window_block.ravel()\n        arr = np.fft.rfft(np.nan_to_num(flat))\n        mags = np.abs(arr)\n        # assign first FFT_N non-dc mags\n        for k in range(1, min(len(mags), fft_n+1)):\n            df.at[i, f'fft_{k}'] = mags[k]\n    # winsorize numeric columns to reduce extreme outliers\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    for c in numeric_cols:\n        df[c] = winsorize_mad(df[c], thresh=4.0)\n    # basic final impute\n    df = basic_impute(df)\n    return df\n\n# usage:\ndata = add_basic_ts_features(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:09:56.958411Z","iopub.execute_input":"2025-11-01T21:09:56.958686Z","iopub.status.idle":"2025-11-01T21:10:02.962668Z","shell.execute_reply.started":"2025-11-01T21:09:56.958666Z","shell.execute_reply":"2025-11-01T21:10:02.962052Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"data.isna().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:10:07.695263Z","iopub.execute_input":"2025-11-01T21:10:07.695534Z","iopub.status.idle":"2025-11-01T21:10:07.738083Z","shell.execute_reply.started":"2025-11-01T21:10:07.695512Z","shell.execute_reply":"2025-11-01T21:10:07.737483Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"# === 5) Signatures: sliding-window per date_id using signatory or iisignature ===\n# Signatures will produce one feature vector per date_id (signature of preceding SIGNATURE_WINDOW rows of selected features)\n\n# preferred: signatory (GPU+torch); fallback: iisignature (C)\ntry:\n    import torch, signatory\n    SIGNATORY_OK = True\nexcept Exception:\n    SIGNATORY_OK = False\n\ntry:\n    import iisignature\n    IISIGNATURE_OK = True\nexcept Exception:\n    IISIGNATURE_OK = False\n\ndef compute_signatures_by_date(df, feature_cols, window=SIGNATURE_WINDOW, depth=SIGNATURE_DEPTH, id_col='date_id'):\n    \"\"\"\n    Returns DataFrame with columns [date_id, sig_0, sig_1, ...] where each row is the signature\n    of the trailing window of 'feature_cols' ending at that date_id.\n    If insufficient history, signature is NaN and will be dropped later.\n    \"\"\"\n    sig_rows = []\n    # ensure sorted\n    df = df.sort_values(id_col).reset_index(drop=True)\n    feats = df[feature_cols].values.astype(np.float32)\n    n, d = feats.shape\n    if SIGNATORY_OK:\n        # compute per-date sliding window in a loop (vectorization possible but more complex)\n        for i in range(n):\n            start = i - window + 1\n            if start < 0:\n                sig_rows.append((df.at[i, id_col], None))\n                continue\n            seg = torch.tensor(feats[start:i+1][None], device='cpu')  # shape (1, L, d)\n            try:\n                sig = signatory.signature(seg, depth=depth)[0].cpu().numpy()\n            except Exception:\n                sig = None\n            sig_rows.append((df.at[i, id_col], sig))\n    elif IISIGNATURE_OK:\n        for i in range(n):\n            start = i - window + 1\n            if start < 0:\n                sig_rows.append((df.at[i, id_col], None))\n                continue\n            seg = feats[start:i+1]\n            try:\n                sig = iisignature.sig(seg, depth)\n            except Exception:\n                sig = None\n            sig_rows.append((df.at[i, id_col], sig))\n    else:\n        raise RuntimeError(\"No signature backend available (install signatory or iisignature).\")\n\n    # build DataFrame, drop None rows\n    valid = [(idx, s) for idx, s in sig_rows if s is not None]\n    if len(valid) == 0:\n        raise RuntimeError(\"No signatures computed (insufficient history). Consider reducing window.\")\n    feat_dim = valid[0][1].shape[0]\n    cols = ['date_id'] + [f'sig_{i}' for i in range(feat_dim)]\n    out = pd.DataFrame([ [idx] + list(s) for idx, s in valid ], columns=cols)\n    return out\n\n# Usage:\nselected_cols = [c for c in data.columns if c.startswith(('M','E','I','P','V','S','MOM'))][:10]  # choose top K to limit dimension\nsig_df = compute_signatures_by_date(data, selected_cols, window=30, depth=3)\ndata = data.merge(sig_df, on='date_id', how='left')\ndata = basic_impute(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:10:10.567933Z","iopub.execute_input":"2025-11-01T21:10:10.568667Z","iopub.status.idle":"2025-11-01T21:10:18.070160Z","shell.execute_reply.started":"2025-11-01T21:10:10.568639Z","shell.execute_reply":"2025-11-01T21:10:18.069548Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"data.isna().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:10:27.852135Z","iopub.execute_input":"2025-11-01T21:10:27.852821Z","iopub.status.idle":"2025-11-01T21:10:27.902980Z","shell.execute_reply.started":"2025-11-01T21:10:27.852797Z","shell.execute_reply":"2025-11-01T21:10:27.902274Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"# === 6) Optional TDA (giotto-tda): persistence image per trailing window ===\n# This block is optional and slower. It computes a persistence image for each trailing window of multivariate features.\n\ntry:\n    from gtda.homology import VietorisRipsPersistence\n    from gtda.diagrams import PersistenceImage\n    GIOTTO_OK = True\nexcept Exception:\n    GIOTTO_OK = False\n\ndef compute_persistence_images_by_date(df, feature_cols, window=SIGNATURE_WINDOW, id_col='date_id'):\n    if not GIOTTO_OK:\n        raise RuntimeError(\"giotto-tda not installed. Install or skip TDA.\")\n    pr = VietorisRipsPersistence(metric='euclidean', homology_dimensions=[0,1])\n    pimg = PersistenceImage()\n    rows = []\n    df = df.sort_values(id_col).reset_index(drop=True)\n    feats = df[feature_cols].values\n    n = len(df)\n    for i in range(n):\n        start = i - window + 1\n        if start < 0:\n            rows.append((df.at[i, id_col], None))\n            continue\n        block = feats[start:i+1]\n        if block.shape[0] < 3:\n            rows.append((df.at[i, id_col], None))\n            continue\n        diagrams = pr.fit_transform(block[None])    # returns array shape (1, ... )\n        image = pimg.fit_transform(diagrams)[0].ravel()\n        rows.append((df.at[i, id_col], image))\n    valid = [(idx,img) for idx,img in rows if img is not None]\n    if len(valid) == 0:\n        raise RuntimeError(\"No persistence images computed.\")\n    dim = valid[0][1].shape[0]\n    cols = ['date_id'] + [f'pi_{i}' for i in range(dim)]\n    out = pd.DataFrame([ [idx] + list(img) for idx,img in valid ], columns=cols)\n    return out\n\n# Usage:\n# pi_df = compute_persistence_images_by_date(data, selected_cols, window=30)\n# data = data.merge(pi_df, on='date_id', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:10:48.411806Z","iopub.execute_input":"2025-11-01T21:10:48.412110Z","iopub.status.idle":"2025-11-01T21:10:48.420560Z","shell.execute_reply.started":"2025-11-01T21:10:48.412068Z","shell.execute_reply":"2025-11-01T21:10:48.419783Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"# === 7) Assemble final training set and features list ===\ndef prepare_features(data, signature_cols_keep_k=200):\n    df = data.copy()\n    # Basic features: all numeric except target and identifiers\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    ignore = ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns', 'is_train']\n    numeric_cols = [c for c in numeric_cols if c not in ignore]\n\n    # Optionally pick top-K signature columns (sig_*)\n    sig_cols = [c for c in numeric_cols if c.startswith('sig_')]\n    if len(sig_cols) > signature_cols_keep_k:\n        # simple heuristic: keep first K\n        sig_cols = sig_cols[:signature_cols_keep_k]\n\n    pi_cols = [c for c in numeric_cols if c.startswith('pi_')]\n    # final features: numeric base + signatures + persistence images\n    base = [c for c in numeric_cols if not (c.startswith('sig_') or c.startswith('pi_'))]\n    features = base + sig_cols + pi_cols\n    # remove date_id if present\n    if 'date_id' in features:\n        features.remove('date_id')\n    return df, features\n\n# usage:\n# after computing signatures/tda and merging into data:\ndata, features = prepare_features(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:10:49.010150Z","iopub.execute_input":"2025-11-01T21:10:49.010432Z","iopub.status.idle":"2025-11-01T21:10:49.124462Z","shell.execute_reply.started":"2025-11-01T21:10:49.010410Z","shell.execute_reply":"2025-11-01T21:10:49.123835Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:11:00.326704Z","iopub.execute_input":"2025-11-01T21:11:00.326949Z","iopub.status.idle":"2025-11-01T21:11:00.345134Z","shell.execute_reply.started":"2025-11-01T21:11:00.326931Z","shell.execute_reply":"2025-11-01T21:11:00.344505Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"      date_id  D1  D2  D3  D4  D5  D6  D7  D8  D9  ...  sig_1100  sig_1101  \\\n0           0   0   0   0   1   1   0   0   0   1  ...  0.000000  0.000000   \n1           1   0   0   0   1   1   0   0   0   1  ...  0.000000  0.000000   \n2           2   0   0   0   1   0   0   0   0   1  ...  0.000000  0.000000   \n3           3   0   0   0   1   0   0   0   0   0  ...  0.000000  0.000000   \n4           4   0   0   0   1   0   0   0   0   0  ...  0.000000  0.000000   \n...       ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...       ...       ...   \n9015     8988   0   0   0   0   0   0   0   0   0  ... -0.000055 -0.000007   \n9016     8989   0   0   0   0   0   0   0   0   0  ... -0.000063 -0.000009   \n9017     8989   0   0   0   0   0   0   0   0   0  ... -0.000061 -0.000008   \n9018     8989   0   0   0   0   0   0   0   0   0  ... -0.000063 -0.000009   \n9019     8989   0   0   0   0   0   0   0   0   0  ... -0.000061 -0.000008   \n\n      sig_1102  sig_1103  sig_1104  sig_1105  sig_1106  sig_1107  sig_1108  \\\n0     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n1     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n2     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n3     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n4     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n...        ...       ...       ...       ...       ...       ...       ...   \n9015  0.000058  0.000058  0.000014  0.000014 -0.000150  0.000626  0.000170   \n9016  0.000056  0.000056  0.000012  0.000012 -0.000146  0.000608  0.000165   \n9017  0.000055  0.000055  0.000012  0.000012 -0.000144  0.000591  0.000161   \n9018  0.000056  0.000056  0.000012  0.000012 -0.000146  0.000608  0.000165   \n9019  0.000055  0.000055  0.000012  0.000012 -0.000144  0.000591  0.000161   \n\n      sig_1109  \n0     0.000000  \n1     0.000000  \n2     0.000000  \n3     0.000000  \n4     0.000000  \n...        ...  \n9015 -0.000106  \n9016 -0.000106  \n9017 -0.000103  \n9018 -0.000106  \n9019 -0.000103  \n\n[9020 rows x 2092 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_id</th>\n      <th>D1</th>\n      <th>D2</th>\n      <th>D3</th>\n      <th>D4</th>\n      <th>D5</th>\n      <th>D6</th>\n      <th>D7</th>\n      <th>D8</th>\n      <th>D9</th>\n      <th>...</th>\n      <th>sig_1100</th>\n      <th>sig_1101</th>\n      <th>sig_1102</th>\n      <th>sig_1103</th>\n      <th>sig_1104</th>\n      <th>sig_1105</th>\n      <th>sig_1106</th>\n      <th>sig_1107</th>\n      <th>sig_1108</th>\n      <th>sig_1109</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9015</th>\n      <td>8988</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.000055</td>\n      <td>-0.000007</td>\n      <td>0.000058</td>\n      <td>0.000058</td>\n      <td>0.000014</td>\n      <td>0.000014</td>\n      <td>-0.000150</td>\n      <td>0.000626</td>\n      <td>0.000170</td>\n      <td>-0.000106</td>\n    </tr>\n    <tr>\n      <th>9016</th>\n      <td>8989</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.000063</td>\n      <td>-0.000009</td>\n      <td>0.000056</td>\n      <td>0.000056</td>\n      <td>0.000012</td>\n      <td>0.000012</td>\n      <td>-0.000146</td>\n      <td>0.000608</td>\n      <td>0.000165</td>\n      <td>-0.000106</td>\n    </tr>\n    <tr>\n      <th>9017</th>\n      <td>8989</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.000061</td>\n      <td>-0.000008</td>\n      <td>0.000055</td>\n      <td>0.000055</td>\n      <td>0.000012</td>\n      <td>0.000012</td>\n      <td>-0.000144</td>\n      <td>0.000591</td>\n      <td>0.000161</td>\n      <td>-0.000103</td>\n    </tr>\n    <tr>\n      <th>9018</th>\n      <td>8989</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.000063</td>\n      <td>-0.000009</td>\n      <td>0.000056</td>\n      <td>0.000056</td>\n      <td>0.000012</td>\n      <td>0.000012</td>\n      <td>-0.000146</td>\n      <td>0.000608</td>\n      <td>0.000165</td>\n      <td>-0.000106</td>\n    </tr>\n    <tr>\n      <th>9019</th>\n      <td>8989</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.000061</td>\n      <td>-0.000008</td>\n      <td>0.000055</td>\n      <td>0.000055</td>\n      <td>0.000012</td>\n      <td>0.000012</td>\n      <td>-0.000144</td>\n      <td>0.000591</td>\n      <td>0.000161</td>\n      <td>-0.000103</td>\n    </tr>\n  </tbody>\n</table>\n<p>9020 rows × 2092 columns</p>\n</div>"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"# === Neural Network Model Definitions ===\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nimport tensorflow as tf\n\ndef build_deep_residual_model(input_dim):\n    \"\"\"Deep Residual Network with Dropout\"\"\"\n    inputs = keras.Input(shape=(input_dim,))\n    \n    # Initial dense layer\n    x = layers.Dense(256, activation='relu')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Residual blocks\n    for units in [128, 128, 64]:\n        residual = x\n        x = layers.Dense(units, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(0.2)(x)\n        x = layers.Dense(units, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        if residual.shape[-1] == units:\n            x = layers.Add()([x, residual])\n        x = layers.Dropout(0.2)(x)\n    \n    # Output\n    outputs = layers.Dense(1)(x)\n    \n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse', metrics=['mae'])\n    return model\n\ndef build_wide_deep_model(input_dim):\n    \"\"\"Wide & Deep Architecture\"\"\"\n    inputs = keras.Input(shape=(input_dim,))\n    \n    # Wide component (linear)\n    wide = layers.Dense(32, activation='relu')(inputs)\n    \n    # Deep component\n    deep = layers.Dense(256, activation='relu')(inputs)\n    deep = layers.BatchNormalization()(deep)\n    deep = layers.Dropout(0.3)(deep)\n    deep = layers.Dense(128, activation='relu')(deep)\n    deep = layers.BatchNormalization()(deep)\n    deep = layers.Dropout(0.2)(deep)\n    deep = layers.Dense(64, activation='relu')(deep)\n    deep = layers.Dropout(0.2)(deep)\n    \n    # Combine wide and deep\n    combined = layers.Concatenate()([wide, deep])\n    combined = layers.Dense(32, activation='relu')(combined)\n    outputs = layers.Dense(1)(combined)\n    \n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse', metrics=['mae'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:11:06.968311Z","iopub.execute_input":"2025-11-01T21:11:06.968590Z","iopub.status.idle":"2025-11-01T21:11:06.977564Z","shell.execute_reply.started":"2025-11-01T21:11:06.968568Z","shell.execute_reply":"2025-11-01T21:11:06.976994Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# === 8) Model training: TimeSeries CV + DNN + XGB OOF ===\ndef train_oof_dnn_xgb(df, features, target='forward_returns', date_col='date_id', n_splits=N_SPLITS, gpu=True):\n    \"\"\"Train DNN and XGBoost with out-of-fold predictions\"\"\"\n    df = df.sort_values(date_col).reset_index(drop=True)\n    idx = df.index.values\n    n = len(df)\n    tss = TimeSeriesSplit(n_splits=n_splits)\n    \n    oof_dnn = np.zeros(n)\n    oof_xgb = np.zeros(n)\n    \n    # XGBoost params\n    xgb_params = dict(\n        objective='reg:squarederror', \n        eval_metric='rmse', \n        learning_rate=0.02,\n        max_depth=8, \n        subsample=0.8, \n        colsample_bytree=0.8, \n        tree_method='gpu_hist' if gpu else 'hist',\n        verbose=5\n    )\n    \n    # Only train on rows that have target (train portion)\n    train_mask = ~df[target].isna()\n    train_idx = np.where(train_mask)[0]\n    splits = list(tss.split(train_idx))\n    print('show splits#############', len(splits))\n    \n    for fold, (t_idx_local, v_idx_local) in enumerate(splits):\n        print(f\"\\n=== Fold {fold+1}/{len(splits)} ===\")\n        t_idx = train_idx[t_idx_local]\n        v_idx = train_idx[v_idx_local]\n        \n        X_tr, X_val = df.iloc[t_idx][features], df.iloc[v_idx][features]\n        y_tr, y_val = df.iloc[t_idx][target].values, df.iloc[v_idx][target].values\n        \n        # === Deep Neural Network ===\n        print(\"Training DNN...\")\n        model_dnn = build_deep_residual_model(len(features))\n        \n        early_stop = callbacks.EarlyStopping(\n            monitor='val_loss', \n            patience=30, \n            restore_best_weights=True,\n            verbose=1\n        )\n        reduce_lr = callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.5, \n            patience=15, \n            min_lr=1e-6,\n            verbose=1\n        )\n        \n        history = model_dnn.fit(\n            X_tr.values, y_tr,\n            validation_data=(X_val.values, y_val),\n            epochs=500,\n            batch_size=256,\n            callbacks=[early_stop, reduce_lr],\n            verbose=0\n        )\n        \n        oof_dnn[v_idx] = model_dnn.predict(X_val.values, verbose=0).flatten()\n        print(f\"DNN Fold {fold+1} val loss: {min(history.history['val_loss']):.6f}\")\n        \n        # === XGBoost ===\n        print(\"Training XGBoost...\")\n        dtrain_x = xgb.DMatrix(X_tr, label=y_tr)\n        dvalid_x = xgb.DMatrix(X_val, label=y_val)\n        \n        model_xgb = xgb.train(\n            xgb_params, \n            dtrain_x, \n            num_boost_round=10000, \n            evals=[(dvalid_x, 'valid')],\n            early_stopping_rounds=200,\n            verbose_eval=500\n        )\n        \n        oof_xgb[v_idx] = model_xgb.predict(dvalid_x, iteration_range=(0, model_xgb.best_iteration))\n        \n        # Cleanup\n        del model_dnn, model_xgb, dtrain_x, dvalid_x\n        keras.backend.clear_session()\n        gc.collect()\n    \n    # Blend simple average for OOF evaluation\n    oof_blend = 0.5 * oof_dnn + 0.5 * oof_xgb\n    \n    # Compute OOF on training rows\n    oof_rmse = mean_squared_error(df.loc[train_idx, target], oof_blend[train_idx], squared=False)\n    print(\"\\n\" + \"=\"*50)\n    print(f\"OOF RMSE (DNN): {mean_squared_error(df.loc[train_idx, target], oof_dnn[train_idx], squared=False):.6f}\")\n    print(f\"OOF RMSE (XGB): {mean_squared_error(df.loc[train_idx, target], oof_xgb[train_idx], squared=False):.6f}\")\n    print(f\"OOF RMSE (blend): {oof_rmse:.6f}\")\n    print(\"=\"*50)\n    \n    return oof_dnn, oof_xgb, oof_blend, oof_rmse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:11:08.148302Z","iopub.execute_input":"2025-11-01T21:11:08.148553Z","iopub.status.idle":"2025-11-01T21:11:08.159327Z","shell.execute_reply.started":"2025-11-01T21:11:08.148533Z","shell.execute_reply":"2025-11-01T21:11:08.158704Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# === 9) Meta-stacker (Ridge) and full-train/predict helpers ===\ndef train_meta_and_full_predict(df, features, oof_preds, target='forward_returns', date_col='date_id', gpu=True):\n    \"\"\"Train meta-model (Ridge) and full models for final predictions\"\"\"\n    \n    df = df.sort_values(date_col).reset_index(drop=True)\n    train_mask = ~df[target].isna()\n    train_idx = np.where(train_mask)[0]\n    \n    # Prepare meta features\n    if isinstance(oof_preds, dict):\n        X_meta = np.vstack([oof_preds[k] for k in sorted(oof_preds.keys())]).T\n    else:\n        X_meta = oof_preds\n    \n    # Train meta-model (Ridge)\n    print(\"\\n=== Training Meta-Model (Ridge) ===\")\n    meta = Ridge(alpha=1.0)\n    meta.fit(X_meta[train_idx], df.loc[train_idx, target].values)\n    oof_meta = meta.predict(X_meta)\n    meta_rmse = mean_squared_error(df.loc[train_idx, target].values, oof_meta[train_idx], squared=False)\n    print(f\"Meta OOF RMSE: {meta_rmse:.6f}\")\n    \n    # Full-train on all training rows\n    X_full = df.loc[train_idx, features]\n    y_full = df.loc[train_idx, target].values\n    \n    # === Wide & Deep Neural Network (full train) ===\n    print(\"\\n=== Training Full Wide-Deep DNN ===\")\n    model_dnn = build_wide_deep_model(len(features))\n    \n    reduce_lr = callbacks.ReduceLROnPlateau(\n        monitor='loss', \n        factor=0.5, \n        patience=20, \n        min_lr=1e-6,\n        verbose=1\n    )\n    \n    history = model_dnn.fit(\n        X_full.values, y_full,\n        epochs=300,\n        batch_size=256,\n        callbacks=[reduce_lr],\n        verbose=1\n    )\n    \n    # === XGBoost (full train) ===\n    print(\"\\n=== Training Full XGBoost ===\")\n    xgb_params = dict(\n        objective='reg:squarederror', \n        eval_metric='rmse', \n        learning_rate=0.02,\n        max_depth=8, \n        subsample=0.8, \n        colsample_bytree=0.8, \n        tree_method='gpu_hist' if gpu else 'hist'\n    )\n    dtrain = xgb.DMatrix(X_full, label=y_full)\n    model_xgb = xgb.train(xgb_params, dtrain, num_boost_round=2000, verbose_eval=500)\n    \n    # Predict for all rows (train + test)\n    print(\"\\n=== Generating Predictions ===\")\n    all_X = df[features]\n    \n    pred_dnn_all = model_dnn.predict(all_X.values, verbose=0).flatten()\n    pred_xgb_all = model_xgb.predict(xgb.DMatrix(all_X))\n    \n    # Stack and meta-predict\n    stack_input = np.vstack([pred_dnn_all, pred_xgb_all]).T\n    pred_meta_all = meta.predict(stack_input)\n    \n    # Add predictions to dataframe\n    df['pred_dnn'] = pred_dnn_all\n    df['pred_xgb'] = pred_xgb_all\n    df['pred_meta'] = pred_meta_all\n    \n    print(\"\\n=== Training Complete ===\")\n    return df, model_dnn, model_xgb, meta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:11:17.217910Z","iopub.execute_input":"2025-11-01T21:11:17.218193Z","iopub.status.idle":"2025-11-01T21:11:17.227106Z","shell.execute_reply.started":"2025-11-01T21:11:17.218174Z","shell.execute_reply":"2025-11-01T21:11:17.226378Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"# === TRAINING CELL - runs once when notebook starts ===\nimport joblib\nimport gc\n\ndef train_and_save_ensemble():\n    \"\"\"Complete training pipeline with DNN + XGBoost + Ridge meta-model\"\"\"\n    \n    print(\"=\"*60)\n    print(\"Starting Ensemble Training Pipeline\")\n    print(\"=\"*60)\n    \n    # 1. Feature Engineering\n    print(\"\\n=== Step 1: Feature Engineering ===\")\n    data_fe = add_basic_ts_features(data, windows=(5,20,60), fft_n=FFT_N)\n    \n    # Feature candidates\n    feat_candidates = [c for c in data_fe.columns if c.startswith(('M','E','I','P','V','S','MOM'))]\n    feat_candidates = sorted(feat_candidates)[:12]\n    \n    # 2. Signatures (optional)\n    print(\"\\n=== Step 2: Computing Signatures ===\")\n    try:\n        sig_df = compute_signatures_by_date(\n            data_fe, \n            feat_candidates, \n            window=SIGNATURE_WINDOW, \n            depth=SIGNATURE_DEPTH\n        )\n        data_fe = data_fe.merge(sig_df, on='date_id', how='left')\n        print(\"Signatures computed successfully\")\n    except Exception as e:\n        print(f\"Signature step skipped: {e}\")\n    \n    # 3. Final feature preparation\n    print(\"\\n=== Step 3: Preparing Features ===\")\n    data_final, features = prepare_features(data_fe, signature_cols_keep_k=200)\n    print(f\"Total features: {len(features)}\")\n    \n    # 4. Train OOF models (DNN + XGBoost)\n    print(\"\\n=== Step 4: Training OOF Models ===\")\n    oof_dnn, oof_xgb, oof_blend, oof_rmse = train_oof_dnn_xgb(\n        data_final, \n        features, \n        target=TARGET, \n        n_splits=N_SPLITS, \n        gpu=True\n    )\n    \n    # 5. Meta-stacking with Ridge\n    print(\"\\n=== Step 5: Meta-Stacking ===\")\n    oof_preds = {'dnn': oof_dnn, 'xgb': oof_xgb}\n    df_preds, model_dnn, model_xgb, meta = train_meta_and_full_predict(\n        data_final, \n        features, \n        oof_preds, \n        target=TARGET, \n        gpu=True\n    )\n    \n    # 6. Save entire ensemble\n    print(\"\\n=== Step 6: Saving Ensemble ===\")\n    ensemble = {\n        'model_dnn': model_dnn,\n        'model_xgb': model_xgb, \n        'meta_model': meta,\n        'features': features,\n        'feature_processor': None,  # Add if you have feature preprocessing\n        'data_final': data_final,   # For reference\n        'oof_rmse': oof_rmse\n    }\n    \n    joblib.dump(ensemble, 'ensemble_model.pkl')\n    print(\"\\n\" + \"=\"*60)\n    print(\"Ensemble trained and saved successfully!\")\n    print(f\"Final OOF RMSE: {oof_rmse:.6f}\")\n    print(\"=\"*60)\n    \n    return ensemble\n\n# Train immediately\ntrained_ensemble = train_and_save_ensemble()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:11:24.986153Z","iopub.execute_input":"2025-11-01T21:11:24.986630Z","iopub.status.idle":"2025-11-01T21:35:58.942951Z","shell.execute_reply.started":"2025-11-01T21:11:24.986608Z","shell.execute_reply":"2025-11-01T21:35:58.942158Z"}},"outputs":[{"name":"stdout","text":"============================================================\nStarting Ensemble Training Pipeline\n============================================================\n\n=== Step 1: Feature Engineering ===\n\n=== Step 2: Computing Signatures ===\nSignatures computed successfully\n\n=== Step 3: Preparing Features ===\nTotal features: 9034\n\n=== Step 4: Training OOF Models ===\nshow splits############# 5\n\n=== Fold 1/5 ===\nTraining DNN...\n\nEpoch 125: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 148: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 163: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\nEpoch 178: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 180: early stopping\nRestoring model weights from the end of the best epoch: 150.\nDNN Fold 1 val loss: 0.000301\nTraining XGBoost...\n[0]\tvalid-rmse:0.01073\n[200]\tvalid-rmse:0.01084\n\n=== Fold 2/5 ===\nTraining DNN...\n\nEpoch 84: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 99: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 114: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\nEpoch 129: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\nEpoch 144: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n\nEpoch 159: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n\nEpoch 174: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n\nEpoch 189: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\nEpoch 190: early stopping\nRestoring model weights from the end of the best epoch: 160.\nDNN Fold 2 val loss: 0.000085\nTraining XGBoost...\n[0]\tvalid-rmse:0.00897\n[205]\tvalid-rmse:0.00909\n\n=== Fold 3/5 ===\nTraining DNN...\n\nEpoch 66: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 81: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 96: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\nEpoch 111: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\nEpoch 126: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n\nEpoch 141: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n\nEpoch 156: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n\nEpoch 171: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\nEpoch 172: early stopping\nRestoring model weights from the end of the best epoch: 142.\nDNN Fold 3 val loss: 0.000108\nTraining XGBoost...\n[0]\tvalid-rmse:0.01034\n[200]\tvalid-rmse:0.01048\n\n=== Fold 4/5 ===\nTraining DNN...\n\nEpoch 46: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 61: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 76: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\nEpoch 91: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\nEpoch 106: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n\nEpoch 121: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n\nEpoch 136: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n\nEpoch 151: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\nEpoch 160: early stopping\nRestoring model weights from the end of the best epoch: 130.\nDNN Fold 4 val loss: 0.000062\nTraining XGBoost...\n[0]\tvalid-rmse:0.00784\n[204]\tvalid-rmse:0.00815\n\n=== Fold 5/5 ===\nTraining DNN...\n\nEpoch 45: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 60: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 75: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\nEpoch 90: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\nEpoch 105: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\nEpoch 118: early stopping\nRestoring model weights from the end of the best epoch: 88.\nDNN Fold 5 val loss: 0.000087\nTraining XGBoost...\n[0]\tvalid-rmse:0.00930\n[200]\tvalid-rmse:0.00945\n\n==================================================\nOOF RMSE (DNN): 0.010733\nOOF RMSE (XGB): 0.009179\nOOF RMSE (blend): 0.009570\n==================================================\n\n=== Step 5: Meta-Stacking ===\n\n=== Training Meta-Model (Ridge) ===\nMeta OOF RMSE: 0.009130\n\n=== Training Full Wide-Deep DNN ===\nEpoch 1/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 88ms/step - loss: 3.0831 - mae: 1.1750 - learning_rate: 0.0010\nEpoch 2/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1932 - mae: 0.3384 - learning_rate: 0.0010\nEpoch 3/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1043 - mae: 0.2489 - learning_rate: 0.0010\nEpoch 4/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0651 - mae: 0.1978 - learning_rate: 0.0010\nEpoch 5/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0454 - mae: 0.1659 - learning_rate: 0.0010\nEpoch 6/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0318 - mae: 0.1362 - learning_rate: 0.0010\nEpoch 7/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0231 - mae: 0.1165 - learning_rate: 0.0010\nEpoch 8/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0181 - mae: 0.1024 - learning_rate: 0.0010\nEpoch 9/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0131 - mae: 0.0859 - learning_rate: 0.0010\nEpoch 10/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0113 - mae: 0.0820 - learning_rate: 0.0010\nEpoch 11/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0086 - mae: 0.0712 - learning_rate: 0.0010\nEpoch 12/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0078 - mae: 0.0674 - learning_rate: 0.0010\nEpoch 13/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0067 - mae: 0.0626 - learning_rate: 0.0010\nEpoch 14/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0060 - mae: 0.0593 - learning_rate: 0.0010\nEpoch 15/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - mae: 0.0546 - learning_rate: 0.0010\nEpoch 16/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - mae: 0.0487 - learning_rate: 0.0010\nEpoch 17/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mae: 0.0448 - learning_rate: 0.0010\nEpoch 18/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mae: 0.0414 - learning_rate: 0.0010\nEpoch 19/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mae: 0.0387 - learning_rate: 0.0010\nEpoch 20/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mae: 0.0371 - learning_rate: 0.0010\nEpoch 21/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mae: 0.0345 - learning_rate: 0.0010\nEpoch 22/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019 - mae: 0.0306 - learning_rate: 0.0010\nEpoch 23/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017 - mae: 0.0290 - learning_rate: 0.0010\nEpoch 24/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0264 - learning_rate: 0.0010\nEpoch 25/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0232 - learning_rate: 0.0010\nEpoch 26/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6189e-04 - mae: 0.0190 - learning_rate: 0.0010\nEpoch 27/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.3984e-04 - mae: 0.0166 - learning_rate: 0.0010\nEpoch 28/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.5623e-04 - mae: 0.0148 - learning_rate: 0.0010\nEpoch 29/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.0760e-04 - mae: 0.0125 - learning_rate: 0.0010\nEpoch 30/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8478e-04 - mae: 0.0127 - learning_rate: 0.0010\nEpoch 31/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.4810e-04 - mae: 0.0114 - learning_rate: 0.0010\nEpoch 32/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8235e-04 - mae: 0.0112 - learning_rate: 0.0010\nEpoch 33/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.1474e-04 - mae: 0.0109 - learning_rate: 0.0010\nEpoch 34/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.9661e-04 - mae: 0.0103 - learning_rate: 0.0010\nEpoch 35/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8287e-04 - mae: 0.0099 - learning_rate: 0.0010\nEpoch 36/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.6148e-04 - mae: 0.0094 - learning_rate: 0.0010\nEpoch 37/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4121e-04 - mae: 0.0089 - learning_rate: 0.0010\nEpoch 38/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4340e-04 - mae: 0.0089 - learning_rate: 0.0010\nEpoch 39/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3995e-04 - mae: 0.0088 - learning_rate: 0.0010\nEpoch 40/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2823e-04 - mae: 0.0086 - learning_rate: 0.0010\nEpoch 41/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2655e-04 - mae: 0.0084 - learning_rate: 0.0010\nEpoch 42/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4993e-04 - mae: 0.0092 - learning_rate: 0.0010\nEpoch 43/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3660e-04 - mae: 0.0088 - learning_rate: 0.0010\nEpoch 44/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3734e-04 - mae: 0.0088 - learning_rate: 0.0010\nEpoch 45/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1927e-04 - mae: 0.0083 - learning_rate: 0.0010\nEpoch 46/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.6018e-04 - mae: 0.0098 - learning_rate: 0.0010\nEpoch 47/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.5587e-04 - mae: 0.0095 - learning_rate: 0.0010\nEpoch 48/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3414e-04 - mae: 0.0090 - learning_rate: 0.0010\nEpoch 49/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.7250e-04 - mae: 0.0102 - learning_rate: 0.0010\nEpoch 50/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2679e-04 - mae: 0.0085 - learning_rate: 0.0010\nEpoch 51/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0895e-04 - mae: 0.0080 - learning_rate: 0.0010\nEpoch 52/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2528e-04 - mae: 0.0087 - learning_rate: 0.0010\nEpoch 53/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3903e-04 - mae: 0.0088 - learning_rate: 0.0010\nEpoch 54/300\n\u001b[1m29/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1329e-04 - mae: 0.0081\nEpoch 54: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2480e-04 - mae: 0.0085 - learning_rate: 0.0010\nEpoch 55/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0217e-04 - mae: 0.0076 - learning_rate: 5.0000e-04\nEpoch 56/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0986e-04 - mae: 0.0075 - learning_rate: 5.0000e-04\nEpoch 57/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.6336e-05 - mae: 0.0075 - learning_rate: 5.0000e-04\nEpoch 58/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.8308e-05 - mae: 0.0076 - learning_rate: 5.0000e-04\nEpoch 59/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0141e-04 - mae: 0.0077 - learning_rate: 5.0000e-04\nEpoch 60/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0365e-04 - mae: 0.0078 - learning_rate: 5.0000e-04\nEpoch 61/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0751e-04 - mae: 0.0078 - learning_rate: 5.0000e-04\nEpoch 62/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.4152e-05 - mae: 0.0075 - learning_rate: 5.0000e-04\nEpoch 63/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.8663e-05 - mae: 0.0076 - learning_rate: 5.0000e-04\nEpoch 64/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0032e-04 - mae: 0.0076 - learning_rate: 5.0000e-04\nEpoch 65/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2637e-04 - mae: 0.0086 - learning_rate: 5.0000e-04\nEpoch 66/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0889e-04 - mae: 0.0081 - learning_rate: 5.0000e-04\nEpoch 67/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0829e-04 - mae: 0.0080 - learning_rate: 5.0000e-04\nEpoch 68/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0111e-04 - mae: 0.0077 - learning_rate: 5.0000e-04\nEpoch 69/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0647e-04 - mae: 0.0078 - learning_rate: 5.0000e-04\nEpoch 70/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.0462e-05 - mae: 0.0073 - learning_rate: 5.0000e-04\nEpoch 71/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.5295e-05 - mae: 0.0075 - learning_rate: 5.0000e-04\nEpoch 72/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0154e-04 - mae: 0.0078 - learning_rate: 5.0000e-04\nEpoch 73/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0408e-04 - mae: 0.0080 - learning_rate: 5.0000e-04\nEpoch 74/300\n\u001b[1m34/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1508e-04 - mae: 0.0085\nEpoch 74: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1482e-04 - mae: 0.0085 - learning_rate: 5.0000e-04\nEpoch 75/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.5685e-05 - mae: 0.0076 - learning_rate: 2.5000e-04\nEpoch 76/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.2675e-05 - mae: 0.0073 - learning_rate: 2.5000e-04\nEpoch 77/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7801e-05 - mae: 0.0072 - learning_rate: 2.5000e-04\nEpoch 78/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5153e-05 - mae: 0.0071 - learning_rate: 2.5000e-04\nEpoch 79/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8874e-05 - mae: 0.0072 - learning_rate: 2.5000e-04\nEpoch 80/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.2193e-05 - mae: 0.0073 - learning_rate: 2.5000e-04\nEpoch 81/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.6947e-05 - mae: 0.0076 - learning_rate: 2.5000e-04\nEpoch 82/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6866e-05 - mae: 0.0072 - learning_rate: 2.5000e-04\nEpoch 83/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.1351e-05 - mae: 0.0073 - learning_rate: 2.5000e-04\nEpoch 84/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8684e-05 - mae: 0.0073 - learning_rate: 2.5000e-04\nEpoch 85/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.0401e-05 - mae: 0.0072 - learning_rate: 2.5000e-04\nEpoch 86/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4876e-05 - mae: 0.0071 - learning_rate: 2.5000e-04\nEpoch 87/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5033e-05 - mae: 0.0071 - learning_rate: 2.5000e-04\nEpoch 88/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6967e-05 - mae: 0.0072 - learning_rate: 2.5000e-04\nEpoch 89/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.9610e-05 - mae: 0.0076 - learning_rate: 2.5000e-04\nEpoch 90/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.9885e-05 - mae: 0.0073 - learning_rate: 2.5000e-04\nEpoch 91/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8743e-05 - mae: 0.0072 - learning_rate: 2.5000e-04\nEpoch 92/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.4250e-05 - mae: 0.0072 - learning_rate: 2.5000e-04\nEpoch 93/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7110e-05 - mae: 0.0072 - learning_rate: 2.5000e-04\nEpoch 94/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0814e-04 - mae: 0.0073 - learning_rate: 2.5000e-04\nEpoch 95/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.0947e-05 - mae: 0.0074 - learning_rate: 2.5000e-04\nEpoch 96/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.0289e-05 - mae: 0.0073 - learning_rate: 2.5000e-04\nEpoch 97/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3396e-05 - mae: 0.0070\nEpoch 97: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3460e-05 - mae: 0.0070 - learning_rate: 2.5000e-04\nEpoch 98/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8400e-05 - mae: 0.0070 - learning_rate: 1.2500e-04\nEpoch 99/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4818e-05 - mae: 0.0068 - learning_rate: 1.2500e-04\nEpoch 100/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6286e-05 - mae: 0.0072 - learning_rate: 1.2500e-04\nEpoch 101/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9862e-05 - mae: 0.0073 - learning_rate: 1.2500e-04\nEpoch 102/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3140e-05 - mae: 0.0070 - learning_rate: 1.2500e-04\nEpoch 103/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8344e-05 - mae: 0.0073 - learning_rate: 1.2500e-04\nEpoch 104/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2271e-05 - mae: 0.0069 - learning_rate: 1.2500e-04\nEpoch 105/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9595e-05 - mae: 0.0070 - learning_rate: 1.2500e-04\nEpoch 106/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.3571e-05 - mae: 0.0071 - learning_rate: 1.2500e-04\nEpoch 107/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.2444e-05 - mae: 0.0071 - learning_rate: 1.2500e-04\nEpoch 108/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1608e-05 - mae: 0.0070 - learning_rate: 1.2500e-04\nEpoch 109/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9193e-05 - mae: 0.0072 - learning_rate: 1.2500e-04\nEpoch 110/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8263e-05 - mae: 0.0072 - learning_rate: 1.2500e-04\nEpoch 111/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6463e-05 - mae: 0.0072 - learning_rate: 1.2500e-04\nEpoch 112/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.3317e-05 - mae: 0.0070 - learning_rate: 1.2500e-04\nEpoch 113/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6165e-05 - mae: 0.0071 - learning_rate: 1.2500e-04\nEpoch 114/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6315e-05 - mae: 0.0071 - learning_rate: 1.2500e-04\nEpoch 115/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6651e-05 - mae: 0.0072 - learning_rate: 1.2500e-04\nEpoch 116/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3259e-05 - mae: 0.0069 - learning_rate: 1.2500e-04\nEpoch 117/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5532e-05 - mae: 0.0071\nEpoch 117: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5515e-05 - mae: 0.0071 - learning_rate: 1.2500e-04\nEpoch 118/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4174e-05 - mae: 0.0071 - learning_rate: 6.2500e-05\nEpoch 119/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5810e-05 - mae: 0.0072 - learning_rate: 6.2500e-05\nEpoch 120/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1719e-05 - mae: 0.0069 - learning_rate: 6.2500e-05\nEpoch 121/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4794e-05 - mae: 0.0071 - learning_rate: 6.2500e-05\nEpoch 122/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2382e-05 - mae: 0.0070 - learning_rate: 6.2500e-05\nEpoch 123/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9928e-05 - mae: 0.0069 - learning_rate: 6.2500e-05\nEpoch 124/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9926e-05 - mae: 0.0070 - learning_rate: 6.2500e-05\nEpoch 125/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1304e-05 - mae: 0.0069 - learning_rate: 6.2500e-05\nEpoch 126/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5271e-05 - mae: 0.0069 - learning_rate: 6.2500e-05\nEpoch 127/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2010e-05 - mae: 0.0070 - learning_rate: 6.2500e-05\nEpoch 128/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0520e-05 - mae: 0.0069 - learning_rate: 6.2500e-05\nEpoch 129/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9218e-05 - mae: 0.0069 - learning_rate: 6.2500e-05\nEpoch 130/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4423e-05 - mae: 0.0070 - learning_rate: 6.2500e-05\nEpoch 131/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1968e-05 - mae: 0.0070 - learning_rate: 6.2500e-05\nEpoch 132/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3648e-05 - mae: 0.0070 - learning_rate: 6.2500e-05\nEpoch 133/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4694e-05 - mae: 0.0070 - learning_rate: 6.2500e-05\nEpoch 134/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9290e-05 - mae: 0.0068 - learning_rate: 6.2500e-05\nEpoch 135/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3230e-05 - mae: 0.0070 - learning_rate: 6.2500e-05\nEpoch 136/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2004e-05 - mae: 0.0069 - learning_rate: 6.2500e-05\nEpoch 137/300\n\u001b[1m35/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0582e-05 - mae: 0.0069\nEpoch 137: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0614e-05 - mae: 0.0069 - learning_rate: 6.2500e-05\nEpoch 138/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9885e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 139/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8610e-05 - mae: 0.0070 - learning_rate: 3.1250e-05\nEpoch 140/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1907e-05 - mae: 0.0070 - learning_rate: 3.1250e-05\nEpoch 141/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9489e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 142/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2863e-05 - mae: 0.0068 - learning_rate: 3.1250e-05\nEpoch 143/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.0870e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 144/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6141e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 145/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3544e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 146/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9569e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 147/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3045e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 148/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9065e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 149/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2971e-05 - mae: 0.0070 - learning_rate: 3.1250e-05\nEpoch 150/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3606e-05 - mae: 0.0070 - learning_rate: 3.1250e-05\nEpoch 151/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7847e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 152/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5295e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 153/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1531e-04 - mae: 0.0072 - learning_rate: 3.1250e-05\nEpoch 154/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9741e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 155/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0138e-05 - mae: 0.0068 - learning_rate: 3.1250e-05\nEpoch 156/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2001e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 157/300\n\u001b[1m29/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1547e-05 - mae: 0.0069\nEpoch 157: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1419e-05 - mae: 0.0069 - learning_rate: 3.1250e-05\nEpoch 158/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0956e-05 - mae: 0.0068 - learning_rate: 1.5625e-05\nEpoch 159/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9178e-05 - mae: 0.0068 - learning_rate: 1.5625e-05\nEpoch 160/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7972e-05 - mae: 0.0067 - learning_rate: 1.5625e-05\nEpoch 161/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8538e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 162/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3988e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 163/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9661e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 164/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9779e-05 - mae: 0.0068 - learning_rate: 1.5625e-05\nEpoch 165/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9095e-05 - mae: 0.0068 - learning_rate: 1.5625e-05\nEpoch 166/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2702e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 167/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2425e-05 - mae: 0.0070 - learning_rate: 1.5625e-05\nEpoch 168/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8803e-05 - mae: 0.0068 - learning_rate: 1.5625e-05\nEpoch 169/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3528e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 170/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9775e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 171/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2394e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 172/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8445e-05 - mae: 0.0068 - learning_rate: 1.5625e-05\nEpoch 173/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8292e-05 - mae: 0.0068 - learning_rate: 1.5625e-05\nEpoch 174/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0587e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 175/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0733e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 176/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0928e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 177/300\n\u001b[1m34/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0934e-05 - mae: 0.0069\nEpoch 177: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0877e-05 - mae: 0.0069 - learning_rate: 1.5625e-05\nEpoch 178/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0170e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 179/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.3968e-05 - mae: 0.0070 - learning_rate: 7.8125e-06\nEpoch 180/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0872e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 181/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.0355e-05 - mae: 0.0069 - learning_rate: 7.8125e-06\nEpoch 182/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9265e-05 - mae: 0.0069 - learning_rate: 7.8125e-06\nEpoch 183/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0514e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 184/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9115e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 185/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9606e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 186/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9081e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 187/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7924e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 188/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9754e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 189/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9066e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 190/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8942e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 191/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1743e-05 - mae: 0.0069 - learning_rate: 7.8125e-06\nEpoch 192/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9284e-05 - mae: 0.0070 - learning_rate: 7.8125e-06\nEpoch 193/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9601e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 194/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0278e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 195/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4039e-05 - mae: 0.0070 - learning_rate: 7.8125e-06\nEpoch 196/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8071e-05 - mae: 0.0068 - learning_rate: 7.8125e-06\nEpoch 197/300\n\u001b[1m34/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0468e-05 - mae: 0.0069\nEpoch 197: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.0345e-05 - mae: 0.0069 - learning_rate: 7.8125e-06\nEpoch 198/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6731e-05 - mae: 0.0067 - learning_rate: 3.9063e-06\nEpoch 199/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2057e-05 - mae: 0.0069 - learning_rate: 3.9063e-06\nEpoch 200/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9455e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 201/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8449e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 202/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9008e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 203/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9166e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 204/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7668e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 205/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8134e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 206/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0138e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 207/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0046e-05 - mae: 0.0069 - learning_rate: 3.9063e-06\nEpoch 208/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8160e-05 - mae: 0.0067 - learning_rate: 3.9063e-06\nEpoch 209/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9677e-05 - mae: 0.0069 - learning_rate: 3.9063e-06\nEpoch 210/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9236e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 211/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9501e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 212/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6821e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 213/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8479e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 214/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9810e-05 - mae: 0.0069 - learning_rate: 3.9063e-06\nEpoch 215/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3381e-05 - mae: 0.0070 - learning_rate: 3.9063e-06\nEpoch 216/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8253e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 217/300\n\u001b[1m29/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8883e-05 - mae: 0.0068\nEpoch 217: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8860e-05 - mae: 0.0068 - learning_rate: 3.9063e-06\nEpoch 218/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8508e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 219/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9597e-05 - mae: 0.0069 - learning_rate: 1.9531e-06\nEpoch 220/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9014e-05 - mae: 0.0069 - learning_rate: 1.9531e-06\nEpoch 221/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8652e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 222/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9759e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 223/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2346e-05 - mae: 0.0069 - learning_rate: 1.9531e-06\nEpoch 224/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0953e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 225/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7471e-05 - mae: 0.0067 - learning_rate: 1.9531e-06\nEpoch 226/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1097e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 227/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7368e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 228/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6576e-05 - mae: 0.0067 - learning_rate: 1.9531e-06\nEpoch 229/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9556e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 230/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.9792e-05 - mae: 0.0069 - learning_rate: 1.9531e-06\nEpoch 231/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8061e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 232/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.7988e-05 - mae: 0.0070 - learning_rate: 1.9531e-06\nEpoch 233/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8789e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 234/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8959e-05 - mae: 0.0069 - learning_rate: 1.9531e-06\nEpoch 235/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8555e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 236/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.7283e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 237/300\n\u001b[1m35/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1240e-05 - mae: 0.0068\nEpoch 237: ReduceLROnPlateau reducing learning rate to 1e-06.\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1147e-05 - mae: 0.0068 - learning_rate: 1.9531e-06\nEpoch 238/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8761e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 239/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7857e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 240/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9466e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 241/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7280e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 242/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9537e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 243/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7211e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 244/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4048e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 245/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9674e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 246/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.9118e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 247/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0600e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 248/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8688e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 249/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.0548e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 250/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9130e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 251/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9882e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 252/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8333e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 253/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0346e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 254/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6654e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 255/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6968e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 256/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0126e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 257/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8000e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 258/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8622e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 259/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9796e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 260/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9384e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 261/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7861e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 262/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7774e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 263/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8762e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 264/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6971e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 265/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7068e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 266/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.9488e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 267/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7078e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 268/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9378e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 269/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8480e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 270/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0344e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 271/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.7428e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 272/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9084e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 273/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8766e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 274/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7828e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 275/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8518e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 276/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0681e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 277/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0375e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 278/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8263e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 279/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6911e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 280/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9226e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 281/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2485e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 282/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0349e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 283/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7991e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 284/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8111e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 285/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1332e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 286/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9088e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 287/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.9499e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 288/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7550e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 289/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7151e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 290/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9000e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 291/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9359e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 292/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9993e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 293/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0709e-05 - mae: 0.0069 - learning_rate: 1.0000e-06\nEpoch 294/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7302e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 295/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9852e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 296/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7812e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 297/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.7479e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 298/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9193e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\nEpoch 299/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6216e-05 - mae: 0.0067 - learning_rate: 1.0000e-06\nEpoch 300/300\n\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8511e-05 - mae: 0.0068 - learning_rate: 1.0000e-06\n\n=== Training Full XGBoost ===\n\n=== Generating Predictions ===\n\n=== Training Complete ===\n\n=== Step 6: Saving Ensemble ===\n\n============================================================\nEnsemble trained and saved successfully!\nFinal OOF RMSE: 0.009570\n============================================================\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"# === INFERENCE SETUP ===\nimport os\nimport pandas as pd\nimport polars as pl\nimport kaggle_evaluation.default_inference_server\nimport numpy as np\nimport joblib\n\n# Global ensemble cache\ncached_ensemble = None\n\ndef load_ensemble():\n    \"\"\"Load pre-trained ensemble - called once on first predict\"\"\"\n    global cached_ensemble\n    if cached_ensemble is None:\n        try:\n            cached_ensemble = joblib.load('ensemble_model.pkl')\n            print(\"Ensemble loaded successfully!\")\n        except Exception as e:\n            print(f\"Failed to load from file: {e}\")\n            try:\n                cached_ensemble = trained_ensemble\n                print(\"Using in-memory trained ensemble\")\n            except:\n                print(\"ERROR: No ensemble available!\")\n                raise\n    return cached_ensemble\n\ndef predict(test: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Competition inference function - predicts positions [0, 2]\"\"\"\n    \n    ensemble = load_ensemble()\n    \n    # Convert to pandas\n    test_pd = test.to_pandas()\n    \n    try:\n        # Ensure we have the same features (in same order)\n        features = ensemble['features']\n        \n        # Get feature matrix (handle missing features gracefully)\n        if set(features).issubset(test_pd.columns):\n            X_test = test_pd[features].copy()\n        else:\n            # Create DataFrame with available features\n            X_test = test_pd[[f for f in features if f in test_pd.columns]].copy()\n            \n            # Add missing features with zeros\n            missing_features = set(features) - set(X_test.columns)\n            for f in missing_features:\n                X_test[f] = 0.0\n        \n        # Ensure correct feature order\n        X_test = X_test[features]\n        X_test = X_test.astype(float)\n        print(X_test.dtypes)\n        # Get base model predictions\n        pred_dnn = ensemble['model_dnn'].predict(X_test.values, verbose=0).flatten()\n        pred_xgb = ensemble['model_xgb'].predict(xgb.DMatrix(X_test))\n        \n        # Meta-stack with Ridge\n        stack_input = np.column_stack([pred_dnn, pred_xgb])\n        final_predictions = ensemble['meta_model'].predict(stack_input)\n        \n        # Convert returns to positions [0, 2]\n        # Simple scaling: shift and clip\n        # Adjust this based on your expected return distribution\n        positions = np.clip(final_predictions * 10 + 1, 0, 2)\n        \n    except Exception as e:\n        print(f\"Inference error: {e}, using fallback\")\n        # Fallback: neutral position\n        positions = np.ones(len(test_pd)) * 1.0\n    \n    return pl.DataFrame({'prediction': positions})\n\n# Test inference function (optional)\ndef test_inference():\n    \"\"\"Quick test of inference pipeline\"\"\"\n    print(\"\\n=== Testing Inference ===\")\n    ensemble = load_ensemble()\n    \n    # Create dummy test data\n    n_test = 100\n    test_data = pd.DataFrame(\n        np.random.randn(n_test, len(ensemble['features'])),\n        columns=ensemble['features']\n    )\n    test_pl = pl.from_pandas(test_data)\n    \n    # Test prediction\n    result = predict(test_pl)\n    print(f\"Generated {len(result)} predictions\")\n    print(f\"Prediction range: [{result['prediction'].min():.4f}, {result['prediction'].max():.4f}]\")\n    print(f\"Mean prediction: {result['prediction'].mean():.4f}\")\n    print(\"Inference test completed successfully!\")\n\n# Uncomment to test inference\ntest_inference()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:52:48.178589Z","iopub.execute_input":"2025-11-01T21:52:48.179141Z","iopub.status.idle":"2025-11-01T21:52:51.247538Z","shell.execute_reply.started":"2025-11-01T21:52:48.179115Z","shell.execute_reply":"2025-11-01T21:52:51.246681Z"}},"outputs":[{"name":"stdout","text":"\n=== Testing Inference ===\nEnsemble loaded successfully!\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nGenerated 100 predictions\nPrediction range: [0.4720, 1.4121]\nMean prediction: 0.9530\nInference test completed successfully!\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"# SERVER STARTUP (same as before)\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:52:59.130625Z","iopub.execute_input":"2025-11-01T21:52:59.130892Z","iopub.status.idle":"2025-11-01T21:55:00.915412Z","shell.execute_reply.started":"2025-11-01T21:52:59.130874Z","shell.execute_reply":"2025-11-01T21:55:00.914734Z"}},"outputs":[{"name":"stdout","text":"D1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\nD1           float64\nD2           float64\nD3           float64\nD4           float64\nD5           float64\n              ...   \nsig_195_x    float64\nsig_196_x    float64\nsig_197_x    float64\nsig_198_x    float64\nsig_199_x    float64\nLength: 9034, dtype: object\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"pl.read_parquet(\"/kaggle/working/submission.parquet\").head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:56:02.877659Z","iopub.execute_input":"2025-11-01T21:56:02.877963Z","iopub.status.idle":"2025-11-01T21:56:02.925303Z","shell.execute_reply.started":"2025-11-01T21:56:02.877939Z","shell.execute_reply":"2025-11-01T21:56:02.924700Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"shape: (5, 2)\n┌─────────┬────────────┐\n│ date_id ┆ prediction │\n│ ---     ┆ ---        │\n│ i64     ┆ f64        │\n╞═════════╪════════════╡\n│ 8980    ┆ 0.996973   │\n│ 8981    ┆ 1.001072   │\n│ 8982    ┆ 0.998926   │\n│ 8983    ┆ 1.000259   │\n│ 8984    ┆ 1.00017    │\n└─────────┴────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>prediction</th></tr><tr><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>8980</td><td>0.996973</td></tr><tr><td>8981</td><td>1.001072</td></tr><tr><td>8982</td><td>0.998926</td></tr><tr><td>8983</td><td>1.000259</td></tr><tr><td>8984</td><td>1.00017</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}